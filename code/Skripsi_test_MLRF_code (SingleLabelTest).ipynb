{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QDfgbiR2CcQe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import collections\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import tree as sk_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ho5uyZdSCysy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b35f25-ef2d-4208-ee14-d402bc6d5bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(162, 433) <class 'pandas.core.series.Series'>\n",
            "---------------------------\n",
            "(162, 433) <class 'pandas.core.frame.DataFrame'>\n",
            "---------------------------\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    dataFrame = pd.read_csv(f'amr_datasets_r_emb.csv',sep=\",\")\n",
        "    dataFrame = dataFrame.drop('!accession', axis=1)\n",
        "    dataFrame = dataFrame.drop('line_age',axis=1)\n",
        "    dataFrame = dataFrame.drop('phen_r_emb',axis=1)\n",
        "\n",
        "    dataFrame2 = pd.read_csv(f'amr_datasets_classes_bin.csv',sep=\",\")\n",
        "    dataFrame2 = dataFrame2.drop('!accession', axis=1)\n",
        "    dataFrame2 = dataFrame2.drop('line_age',axis=1)\n",
        "\n",
        "    # dataFrame = pd.DataFrame({\n",
        "    #     \"Outlook\":\n",
        "    #     [\"S\",\"S\",\"O\",\"R\",\"R\",\"R\",\"O\",\"S\",\"S\",\"R\",\"S\",\"O\",\"O\",\"R\"],\n",
        "    #     \"Temp\": \n",
        "    #     [\"H\",\"H\",\"H\",\"M\",\"C\",\"C\",\"C\",\"M\",\"C\",\"M\",\"M\",\"M\",\"H\",\"M\"],\n",
        "    #     \"Humidity\": \n",
        "    #     [\"H\",\"H\",\"H\",\"H\",\"N\",\"N\",\"N\",\"H\",\"N\",\"N\",\"N\",\"H\",\"N\",\"H\"],\n",
        "    #     \"Windy\":\n",
        "    #     [\"F\",\"T\",\"F\",\"F\",\"F\",\"T\",\"T\",\"F\",\"F\",\"F\",\"T\",\"T\",\"F\",\"T\"],\n",
        "    #     \"Play\":\n",
        "    #     [\"F\",\"F\",\"T\",\"T\",\"T\",\"F\",\"T\",\"F\",\"T\",\"T\",\"T\",\"T\",\"T\",\"F\"]\n",
        "    # })\n",
        "\n",
        "    # create LabelEncoder object\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # fit and transform the categorical variable\n",
        "\n",
        "    # for header in column_headers:\n",
        "    for header in dataFrame.columns:\n",
        "      dataFrame[header] = label_encoder.fit_transform(dataFrame[header])\n",
        "\n",
        "    x2, y2 = dataFrame2.iloc[:, :-4], dataFrame2.iloc[:, -4:] #multilabel\n",
        "    x, y = dataFrame.iloc[:, :-1], dataFrame.iloc[:, -1]\n",
        "    print(x.shape,type(y))\n",
        "    print(\"---------------------------\")\n",
        "    print(x2.shape,type(y2))\n",
        "    print(\"---------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy Calculation"
      ],
      "metadata": {
        "id": "D9fuiMEvoQaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marginal entropy"
      ],
      "metadata": {
        "id": "-4N7LiqHzQZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_marginal_ent(y):\n",
        "  mgn_entropys = []\n",
        "  for class_header in y.columns:\n",
        "    y_label = y[class_header].value_counts()\n",
        "    mgn_entropy = 0\n",
        "    for idx in range(len(y_label)):\n",
        "      prob = y_label[idx]/len(y)\n",
        "      mgn_entropy += (prob * math.log2(prob))\n",
        "    mgn_entropys.append(-1*mgn_entropy)\n",
        "    print(f\"marginal entropy {class_header}: {mgn_entropy}\\n\")\n",
        "  return mgn_entropys"
      ],
      "metadata": {
        "id": "8FG6wJ6G4KbH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional entropy"
      ],
      "metadata": {
        "id": "cPYO5qDl4HMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conditional_entropy(x,y):\n",
        "\n",
        "  cond_entropys = []\n",
        "  attrs = x.columns\n",
        "  class_label_name = y.name\n",
        "\n",
        "  for attr in attrs:\n",
        "    \n",
        "    # count all unique value of an attribute\n",
        "    attr_vals = x[attr].value_counts()\n",
        "    cond_entropy = 0\n",
        "\n",
        "    for attr_val in range(len(attr_vals)):\n",
        "\n",
        "      # print attribute name and the selected value \n",
        "      # print(attr, attr_val)\n",
        "\n",
        "      # get index of specific attribute value e.g idx of (Outlook == Sunny)\n",
        "      val_idx = x[x[attr]==attr_val].index\n",
        "      # print(val_idx)\n",
        "\n",
        "      # total count of instance with specific attribute value\n",
        "      val_cnt = len(val_idx)\n",
        "      val_prob = -1*val_cnt/len(y)\n",
        "\n",
        "      # get information of class label based on specific attr value idx\n",
        "      val_labels = y[val_idx].value_counts()\n",
        "      # print(val_labels)\n",
        "      tmp = 0\n",
        "\n",
        "      for val_label_idx, val_label in enumerate(val_labels):\n",
        "        if (val_label == val_cnt):\n",
        "          tmp += 0\n",
        "        else :\n",
        "          val_label_prob = val_label/val_cnt \n",
        "          tmp += (val_label_prob* math.log2(val_label_prob))\n",
        "\n",
        "        # count total label for each attr value\n",
        "        # print(f\"label {val_label_idx} count\" \\\n",
        "        #   f\" in attr {attr}-{attr_val} : {val_label}\"\n",
        "        # )\n",
        "        \n",
        "      cond_entropy += val_prob * tmp\n",
        "      # print(f\"{val_prob * tmp}\\n\")\n",
        "\n",
        "    print(f\"conditional entropy for {class_label_name}|{attr}:\",cond_entropy)\n",
        "    print(\"\\n\")\n",
        "    cond_entropys.append(cond_entropy)\n",
        "\n",
        "  print(cond_entropys)\n",
        "  return cond_entropys"
      ],
      "metadata": {
        "id": "pCwHNp214JVS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1Rhh13ASLSS"
      },
      "source": [
        "# Code below for multiple label dataset (can be multiple value in single label) with joint information gain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Joint Information Gain"
      ],
      "metadata": {
        "id": "OJYdnErx6rDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ohfu4t2Rgaw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "5fd4288c-d265-43bf-8497-ce8f8ba8ab1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-8d57de658f5c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get marginal entropy for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmgn_entropys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_marginal_ent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjoint_cond_entropys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get conditional entropy for each class label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-a1dafdd8465c>\u001b[0m in \u001b[0;36mget_marginal_ent\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_marginal_ent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmgn_entropys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mclass_header\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_header\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmgn_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"
          ]
        }
      ],
      "source": [
        "# Get marginal entropy for each class\n",
        "mgn_entropys = get_marginal_ent(y)\n",
        "\n",
        "joint_cond_entropys = []\n",
        "# Get conditional entropy for each class label\n",
        "for class_header_idx,class_header in enumerate(y.columns):\n",
        "  #y_label = y[class_header].value_counts()\n",
        "  y_label = y[class_header]\n",
        "  joint_cond_entropys.append(get_conditional_entropy(x,y_label))\n",
        "\n",
        "# Get Joint Information gain for all attribute/feature\n",
        "joint_information_gains = []\n",
        "for cond_idx, cond_entropys in enumerate(joint_cond_entropys[0]):\n",
        "  joint_information_gain = 0\n",
        "  for mgn_idx,mgn_entropy in enumerate(mgn_entropys):\n",
        "    joint_information_gain += mgn_entropy-joint_cond_entropys[mgn_idx][cond_idx]\n",
        "  joint_information_gains.append(joint_information_gain)\n",
        "np_joint_information_gains = np.array(joint_information_gains)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"all class marginal entropy:\\n\",len(mgn_entropys),mgn_entropys)\n",
        "print(\"all class conditional entropy:\\n\",np.array(joint_cond_entropys).shape)\n",
        "print(joint_cond_entropys)\n",
        "print(\"joint ig for all attr:\\n\",np.array(joint_information_gains))\n",
        "print(\"max joint ig on attribute\\n\",\n",
        "      x.columns[np_joint_information_gains.argmax()], \"\\nwith joint ig value:\",\n",
        "      np_joint_information_gains[np_joint_information_gains.argmax()]\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "PyQj5SIf-HQY",
        "outputId": "783522de-d984-45aa-f7a6-e6c6bca2eeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all class marginal entropy:\n",
            " 5 [1.5774062828523454, 1.5566567074628228, 1.0, 0.9852281360342515, 0.9402859586706311]\n",
            "all class conditional entropy:\n",
            " (5, 0)\n",
            "[[], [], [], [], []]\n",
            "joint ig for all attr:\n",
            " []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-2659aa5339a5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"joint ig for all attr:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_information_gains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m print(\"max joint ig on attribute\\n\",\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_joint_information_gains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nwith joint ig value:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mnp_joint_information_gains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_joint_information_gains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       )\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sxzbrFbSDWD"
      },
      "source": [
        "# Code below for single label dataset (can be multiple value in single label) with information gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmoDm31eCYF7"
      },
      "outputs": [],
      "source": [
        "print(y.value_counts())\n",
        "print(len(y))\n",
        "mgn_entropy = 0\n",
        "cond_entropys = []\n",
        "for idx in range(len(y.value_counts())):\n",
        "  prob = y.value_counts()[idx]/len(y)\n",
        "  print(prob * math.log2(prob))\n",
        "  mgn_entropy += (prob * math.log2(prob))\n",
        "\n",
        "print(\"marginal entropy: \"+str(-1*mgn_entropy),\"\\n\")\n",
        "\n",
        "# print(x.Outlook.value_counts())\n",
        "# print(x[x.columns[0]])\n",
        "\n",
        "# count number of column in feature spaces\n",
        "attrs = x.columns\n",
        "for attr in range(len(attrs)):\n",
        "\n",
        "  # count all unique value of an attribute\n",
        "  attr_vals = x[x.columns[attr]].value_counts()\n",
        "  cond_entropy = 0\n",
        "\n",
        "  for attr_val in range(len(attr_vals)):\n",
        "    \n",
        "    # print attribute name and the selected value \n",
        "    print(x.columns[attr], attr_val)\n",
        "\n",
        "    # get index of specific attribute value e.g idx of (Outlook == Sunny)\n",
        "    val_idx = x[x[x.columns[attr]]==attr_val].index\n",
        "    print(val_idx)\n",
        "\n",
        "    # total count of instance with specific attribute value\n",
        "    val_cnt = len(val_idx)\n",
        "    val_prob = -1*val_cnt/len(y)\n",
        "\n",
        "    # get information of class label based on specific attr value idx\n",
        "    val_labels = y[val_idx].value_counts()\n",
        "    print(val_labels)\n",
        "    tmp = 0\n",
        "\n",
        "    for val_label_idx, val_label in enumerate(val_labels):\n",
        "      if (val_label == val_cnt):\n",
        "        tmp += 0\n",
        "      else :\n",
        "        val_label_prob = val_label/val_cnt \n",
        "        tmp += (val_label_prob* math.log2(val_label_prob))\n",
        "\n",
        "      # count total label for each attr value\n",
        "      print(f\"label {val_label_idx} count\" \\\n",
        "        f\" in attr {x.columns[attr]}-{attr_val} : {val_label}\"\n",
        "      )\n",
        "    cond_entropy += val_prob * tmp\n",
        "    print(f\"{val_prob * tmp}\\n\")\n",
        "\n",
        "  print(f\"conditional entropy for Class|{x.columns[attr]}:\",cond_entropy)\n",
        "  print(\"\\n\")\n",
        "  cond_entropys.append(cond_entropy)\n",
        "\n",
        "print(cond_entropys)\n",
        "\n",
        "for cond_entropy in cond_entropys:\n",
        "  print(-1*mgn_entropy-cond_entropy)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipEma7srJFAk"
      },
      "source": [
        "# RF Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "mvFzcXT4EQrV"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@Env: Python2.7\n",
        "@Time: 2019/10/24 13:31\n",
        "@Author: zhaoxingfeng\n",
        "@Function：Random Forest（RF），随机森林二分类\n",
        "@Version: V1.2\n",
        "参考文献：\n",
        "[1] UCI. wine[DB/OL].https://archive.ics.uci.edu/ml/machine-learning-databases/wine.\n",
        "\"\"\"\n",
        "\n",
        "class Tree(object):\n",
        "    \"\"\"Define a decision tree\"\"\"\n",
        "    def __init__(self):\n",
        "        self.split_feature = None\n",
        "        self.split_value = None\n",
        "        self.leaf_value = None\n",
        "        self.tree_left = None\n",
        "        self.tree_right = None\n",
        "\n",
        "    def calc_predict_value(self, dataset):\n",
        "        \"\"\"Find the leaf node of the sample through the recursive decision tree\"\"\"\n",
        "        if self.leaf_value is not None:\n",
        "            return self.leaf_value\n",
        "        elif dataset[self.split_feature] <= self.split_value:\n",
        "            return self.tree_left.calc_predict_value(dataset)\n",
        "        else:\n",
        "            return self.tree_right.calc_predict_value(dataset)\n",
        "\n",
        "    def describe_tree(self):\n",
        "        \"\"\"\n",
        "        Print the decision tree in json form, \n",
        "        which is convenient for viewing the tree structure\n",
        "        \"\"\"\n",
        "        if not self.tree_left and not self.tree_right:\n",
        "            leaf_info = \"{leaf_value:\" + str(self.leaf_value) + \"}\"\n",
        "            return leaf_info\n",
        "        left_info = self.tree_left.describe_tree()\n",
        "        right_info = self.tree_right.describe_tree()\n",
        "        tree_structure = \"{split_feature:\" + str(self.split_feature) + \\\n",
        "                         \",split_value:\" + str(self.split_value) + \\\n",
        "                         \",left_tree:\" + left_info + \\\n",
        "                         \",right_tree:\" + right_info + \"}\"\n",
        "        return tree_structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "0SQjkSjvEU1d"
      },
      "outputs": [],
      "source": [
        "class RandomForestClassifier(object):\n",
        "    def __init__(self, n_estimators=10, max_depth=-1, min_samples_split=2, min_samples_leaf=1,\n",
        "                 min_split_gain=0.0, colsample_bytree=None, subsample=0.8, random_state=None,\n",
        "                 classifier_type=\"gini\"):\n",
        "        \"\"\"\n",
        "        Random Forest Parameters\n",
        "         ----------\n",
        "         n_estimators: \n",
        "              number of trees\n",
        "         max_depth: \n",
        "              tree depth, -1 means unlimited depth\n",
        "         min_samples_split: \n",
        "              The minimum number of samples required for node splitting, \n",
        "              the node terminates splitting if it is less than this value\n",
        "         min_samples_leaf: \n",
        "              The minimum sample number of leaf nodes, \n",
        "              less than this value leaves are merged\n",
        "         min_split_gain: \n",
        "              The minimum gain required for splitting, \n",
        "              less than this value the node terminates the split\n",
        "         colsample_bytree: \n",
        "              Column sampling setting, which can be [sqrt, log2]. \n",
        "              sqrt means randomly selecting sqrt(n_features) features,\n",
        "              log2 means to randomly select log(n_features) features, \n",
        "              if set to other, column sampling will not be performed\n",
        "         subsample: \n",
        "              line sampling ratio\n",
        "         random_state: \n",
        "              Random seed, after setting, \n",
        "              the n_estimators sample sets generated each time will not change, \n",
        "              ensuring that the experiment can be repeated\n",
        "         classifier_type:\n",
        "              Select method for node splitting \"gini\" or \"information_gain\"\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth if max_depth != -1 else float('inf')\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.min_split_gain = min_split_gain\n",
        "        self.colsample_bytree = colsample_bytree\n",
        "        self.subsample = subsample\n",
        "        self.random_state = random_state\n",
        "        self.trees = None\n",
        "        self.feature_importances_ = dict()\n",
        "        self.classifier_type = classifier_type\n",
        "\n",
        "    # ERROR DI TARGET MLRF VVVVVVV\n",
        "    def fit(self, dataset, targets):\n",
        "        \"\"\"Model training entry\"\"\"\n",
        "        if type(targets)==pd.core.series.Series:\n",
        "          assert targets.unique().__len__() >= 2, \"There must be two class for targets!\"\n",
        "          targets = targets.to_frame()\n",
        "\n",
        "        # ERROR DI TARGET MLRF     ^^^^^^^^^^^^\n",
        "        if self.random_state:\n",
        "            random.seed(self.random_state)\n",
        "        random_state_stages = random.sample(range(self.n_estimators), self.n_estimators)\n",
        "\n",
        "        # Two column sampling methods\n",
        "        if self.colsample_bytree == \"sqrt\":\n",
        "            self.colsample_bytree = int(len(dataset.columns) ** 0.5)\n",
        "        elif self.colsample_bytree == \"log2\":\n",
        "            self.colsample_bytree = int(math.log(len(dataset.columns)))\n",
        "        else:\n",
        "            self.colsample_bytree = len(dataset.columns)\n",
        "\n",
        "        # Build multiple decision trees in parallel\n",
        "        self.trees = Parallel(n_jobs=-1, verbose=0, backend=\"threading\")(\n",
        "            delayed(self._parallel_build_trees)(dataset, targets, random_state)\n",
        "                for random_state in random_state_stages)\n",
        "        \n",
        "    def _parallel_build_trees(self, dataset, targets, random_state):\n",
        "        \"\"\"\n",
        "        bootstrap has put back sampling to \n",
        "        generate a training sample set and build a decision tree\n",
        "        \"\"\"\n",
        "        subcol_index = random.sample(dataset.columns.tolist(), self.colsample_bytree)\n",
        "        dataset_stage = dataset.sample(n=int(self.subsample * len(dataset)), replace=True, \n",
        "                                        random_state=random_state).reset_index(drop=True)\n",
        "        dataset_stage = dataset_stage.loc[:, subcol_index]\n",
        "        targets_stage = targets.sample(n=int(self.subsample * len(dataset)), replace=True, \n",
        "                                        random_state=random_state).reset_index(drop=True)\n",
        "        tree = self._build_single_tree(dataset_stage, targets_stage, depth=0)\n",
        "\n",
        "        # -------------- PRINT BEST NODE --------------\n",
        "        # print(tree.describe_tree())\n",
        "\n",
        "        return tree\n",
        "\n",
        "    def _build_single_tree(self, dataset, targets, depth):\n",
        "        \"\"\"Recursively build a decision tree\"\"\"\n",
        "        # If the categories of the node \n",
        "        # are all the same/the samples are less than \n",
        "        # the minimum number of samples required for splitting, \n",
        "        # select the category with the most occurrences. \n",
        "        # Termination of division/split\n",
        "        if targets.shape[1]==1:\n",
        "          if targets.shape[1]==1 and \\\n",
        "          ( len(targets[targets.columns[0]].unique()) <= 1 or \n",
        "           dataset.__len__() <= self.min_samples_split ):\n",
        "              tree = Tree()\n",
        "              tree.leaf_value = self.calc_leaf_value(targets[targets.columns[0]])\n",
        "              return tree\n",
        "\n",
        "        if depth < self.max_depth:\n",
        "            best_split_feature, best_split_value, best_split_gain = self.choose_best_feature(dataset, targets)\n",
        "            left_dataset, right_dataset, left_targets, right_targets = \\\n",
        "                self.split_dataset(dataset, targets, best_split_feature, best_split_value)\n",
        "\n",
        "            tree = Tree()\n",
        "            # If after the parent node is split, \n",
        "            # the left leaf node/right leaf node sample is less than \n",
        "            # the set minimum number of leaf node samples, \n",
        "            # the parent node will terminate the split\n",
        "            if left_dataset.__len__() <= self.min_samples_leaf or \\\n",
        "                    right_dataset.__len__() <= self.min_samples_leaf or \\\n",
        "                    best_split_gain <= self.min_split_gain:\n",
        "                tree.leaf_value = self.calc_leaf_value(targets[targets.columns[0]])\n",
        "                return tree\n",
        "            else:\n",
        "                # If this feature is used when splitting, \n",
        "                # the importance of this feature will be increased by 1\n",
        "                self.feature_importances_[best_split_feature] = \\\n",
        "                    self.feature_importances_.get(best_split_feature, 0) + 1\n",
        "\n",
        "                tree.split_feature = best_split_feature\n",
        "                tree.split_value = best_split_value\n",
        "                tree.tree_left = self._build_single_tree(left_dataset, left_targets, depth+1)\n",
        "                tree.tree_right = self._build_single_tree(right_dataset, right_targets, depth+1)\n",
        "                return tree\n",
        "        # If the depth of the tree exceeds the preset value, terminate the split\n",
        "        else:\n",
        "            tree = Tree()\n",
        "            tree.leaf_value = self.calc_leaf_value(targets[targets.columns[0]])\n",
        "            return tree\n",
        "\n",
        "    def choose_best_feature(self, dataset, targets):\n",
        "        \"\"\"\n",
        "        Find the best data set division method, \n",
        "        find the optimal split feature, \n",
        "        split threshold, split gain\n",
        "        \"\"\"\n",
        "        best_split_gain = 1\n",
        "        best_split_feature = None\n",
        "        best_split_value = None\n",
        "\n",
        "        if self.classifier_type == \"gini\":\n",
        "            for feature in dataset.columns:\n",
        "                if dataset[feature].unique().__len__() <= 100:\n",
        "                    unique_values = sorted(dataset[feature].unique().tolist())\n",
        "                # If the dimension feature has too many values, \n",
        "                # select the 100th percentile value as the split threshold to be selected\n",
        "                else:\n",
        "                    unique_values = np.unique([np.percentile(dataset[feature], x)\n",
        "                                              for x in np.linspace(0, 100, 100)]\n",
        "                                              )\n",
        "\n",
        "                # Calculate the splitting gain for the possible splitting thresholds, \n",
        "                # and select the threshold with the largest gain\n",
        "                for split_value in unique_values:\n",
        "                    # get subset of targets that <= split value into left_targets\n",
        "                    left_targets = targets[dataset[feature] <= split_value]\n",
        "                    # get subset of targets that > split value into right_targets\n",
        "                    right_targets = targets[dataset[feature] > split_value]\n",
        "                    split_gain = self.calc_gini(left_targets[targets.columns[0]], right_targets[targets.columns[0]])\n",
        "\n",
        "                    if split_gain < best_split_gain:\n",
        "                        best_split_feature = feature\n",
        "                        best_split_value = split_value\n",
        "                        best_split_gain = split_gain\n",
        "        else : \n",
        "            # Get marginal entropy for each class\n",
        "            mgn_entropys = self.get_marginal_ent(targets)\n",
        "\n",
        "            joint_cond_entropys = []\n",
        "\n",
        "            # Get conditional entropy for each class label\n",
        "            for class_header_idx,class_header in enumerate(targets.columns):\n",
        "              #y_label = y[class_header].value_counts()\n",
        "              y_label = targets[class_header]\n",
        "              cond_entropys =\\\n",
        "                  self.get_conditional_entropy(dataset,y_label)\n",
        "              \n",
        "              joint_cond_entropys.append(\n",
        "                  cond_entropys\n",
        "              )\n",
        "            \n",
        "            # Get Joint Information gain for all attribute/feature\n",
        "            joint_information_gains = []\n",
        "            for cond_idx, cond_entropys in enumerate(joint_cond_entropys[0]):\n",
        "              joint_information_gain = 0\n",
        "              for mgn_idx,mgn_entropy in enumerate(mgn_entropys):\n",
        "                joint_information_gain += mgn_entropy-joint_cond_entropys[mgn_idx][cond_idx]\n",
        "              joint_information_gains.append(joint_information_gain)\n",
        "            \n",
        "            np_joint_information_gains = np.array(joint_information_gains)\n",
        "            best_split_feature = dataset.columns[\n",
        "                np.argmax(np_joint_information_gains)\n",
        "                ]\n",
        "            best_split_infogain = np_joint_information_gains.max()\n",
        "            best_split_value = 0\n",
        "\n",
        "        return best_split_feature, best_split_value, best_split_gain\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_leaf_value(targets):\n",
        "        \"\"\"\n",
        "        Select the category with the most occurrences \n",
        "        in the sample as the value of the leaf node\n",
        "        \"\"\"\n",
        "        label_counts = collections.Counter(targets)\n",
        "        major_label = max(zip(label_counts.values(), label_counts.keys()))\n",
        "        return major_label[1]\n",
        "\n",
        "    # Information Gain METHOD - Conditional Entropy\n",
        "    @staticmethod\n",
        "    def get_conditional_entropy(x,y):\n",
        "\n",
        "        cond_entropys = []\n",
        "        attrs = x.columns\n",
        "        class_label_name = y.name\n",
        "\n",
        "        for attr in attrs:\n",
        "          \n",
        "          # count all unique value of an attribute\n",
        "          attr_vals = x[attr].value_counts()\n",
        "          cond_entropy = []\n",
        "\n",
        "          for attr_val in range(len(attr_vals)):\n",
        "\n",
        "            # print attribute name and the selected value \n",
        "            # print(attr, attr_val)\n",
        "\n",
        "            # get index of specific attribute value e.g idx of (Outlook == Sunny)\n",
        "            val_idx = x[x[attr]==attr_val].index\n",
        "            # print(val_idx)\n",
        "\n",
        "            # total count of instance with specific attribute value\n",
        "            val_cnt = len(val_idx)\n",
        "            val_prob = -1*val_cnt/len(y)\n",
        "\n",
        "            # get information of class label based on specific attr value idx\n",
        "            val_labels = y[val_idx].value_counts()\n",
        "            # print(val_labels)\n",
        "            tmp = 0\n",
        "\n",
        "            for val_label_idx, val_label in enumerate(val_labels):\n",
        "              if (val_label == val_cnt):\n",
        "                tmp += 0\n",
        "              else :\n",
        "                val_label_prob = val_label/val_cnt \n",
        "                tmp += (val_label_prob* math.log2(val_label_prob))\n",
        "\n",
        "              # count total label for each attr value\n",
        "              # print(f\"label {val_label_idx} count\" \\\n",
        "              #   f\" in attr {attr}-{attr_val} : {val_label}\"\n",
        "              # )\n",
        "            \n",
        "            attr_val_ent = val_prob * tmp\n",
        "\n",
        "            cond_entropy.append(attr_val_ent) \n",
        "            # print(f\"{val_prob * tmp}\\n\")\n",
        "\n",
        "          #print(f\"conditional entropy for {class_label_name}|{attr}:\",np.sum(np.array(cond_entropy)))\n",
        "          #print(\"\\n\")\n",
        "          \n",
        "          cond_entropys.append(np.sum(np.array(cond_entropy)))\n",
        "\n",
        "        #print(cond_entropys)\n",
        "        return cond_entropys\n",
        "\n",
        "    # Information Gain METHOD - Marginal Entropy\n",
        "    @staticmethod\n",
        "    def get_marginal_ent(y):\n",
        "        mgn_entropys = []\n",
        "        for class_header in y.columns:\n",
        "          y_label = y[class_header].value_counts()\n",
        "          mgn_entropy = 0\n",
        "          for idx in range(len(y_label)):\n",
        "            prob = y_label[idx]/len(y)\n",
        "            mgn_entropy += (prob * math.log2(prob))\n",
        "          mgn_entropys.append(-1*mgn_entropy)\n",
        "          #print(f\"marginal entropy {class_header}: {mgn_entropy}\\n\")\n",
        "        return mgn_entropys\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_gini(left_targets, right_targets):\n",
        "        \"\"\"\n",
        "        The classification tree uses the Gini index as an \n",
        "        indicator to select the optimal split point\n",
        "        \"\"\"\n",
        "        split_gain = 0\n",
        "        for targets in [left_targets, right_targets]:\n",
        "            gini = 1\n",
        "            # Count how many samples are in each category, \n",
        "            # and then calculate gini\n",
        "            label_counts = collections.Counter(targets)\n",
        "            for key in label_counts:\n",
        "                prob = label_counts[key] * 1.0 / len(targets)\n",
        "                gini -= prob ** 2\n",
        "            split_gain += len(targets) * 1.0 / (len(left_targets) + len(right_targets)) * gini\n",
        "        return split_gain\n",
        "\n",
        "    @staticmethod\n",
        "    def split_dataset(dataset, targets, split_feature, split_value):\n",
        "        \"\"\"\n",
        "        Divide the sample into left and right parts according to the \n",
        "        characteristics and threshold, the left is less than or \n",
        "        equal to the threshold, and the right is greater than the threshold\n",
        "        \"\"\"\n",
        "        left_dataset = dataset[dataset[split_feature] <= split_value]\n",
        "        left_targets = targets[dataset[split_feature] <= split_value]\n",
        "        right_dataset = dataset[dataset[split_feature] > split_value]\n",
        "        right_targets = targets[dataset[split_feature] > split_value]\n",
        "        return left_dataset, right_dataset, left_targets, right_targets\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        \"\"\"Input sample, predict category\"\"\"\n",
        "        res = []\n",
        "        for _, row in dataset.iterrows():\n",
        "            pred_list = []\n",
        "            # Count the prediction results of each tree, \n",
        "            # and select the result with the most occurrences \n",
        "            # as the final category\n",
        "            for tree in self.trees:\n",
        "                pred_list.append(tree.calc_predict_value(row))\n",
        "\n",
        "            pred_label_counts = collections.Counter(pred_list)\n",
        "            pred_label = max(zip(pred_label_counts.values(), pred_label_counts.keys()))\n",
        "            res.append(pred_label[1])\n",
        "        return np.array(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBVAOaooJJX0"
      },
      "source": [
        "# Run here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch Optimization"
      ],
      "metadata": {
        "id": "nRiC8jqZTQik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D32M1uEoEawV",
        "outputId": "944d3dbd-8cd2-4f07-f86f-317079b389c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7024793388429752\n",
            "0.7073170731707317\n",
            "{'pncA_W68G': 1, 'ethA_2_.': 1, 'Planobispora_rosea_EF_Tu_.': 1, 'mshA_1_.': 2, 'embB_1_Y334H': 1, 'pncA_A102V': 1, 'ethA_2_D56Y': 1}\n",
            "0.7107438016528925\n",
            "0.7073170731707317\n",
            "{'pncA_W68L': 1, 'katG_1_S315N': 2, 'pncA_F58L': 1, 'iniA_.': 2, 'embA_.': 2, 'pncA_Q141P': 2, 'ethA_2_C403Y': 1, 'mshA_1_.': 1, 'embB_1_Y334H': 1, 'gidB_.': 1, 'iniB_.': 1, 'Mycobacterium_tuberculosis_ndh_A300P': 1, 'embB_1_G406A': 1, 'pncA_V45G': 1, 'Erm_37__.': 1, 'gidB_D67H': 1, 'rpoB_1_.': 1, 'embB_1_R469P': 1}\n",
            "0.7107438016528925\n",
            "0.6829268292682927\n",
            "{'katG_1_S315N': 2, 'Planobispora_rosea_EF_Tu_.': 2, 'embB_1_G406S': 1, 'pncA_S164P': 1, 'pncA_W68R': 1, 'efpA_.': 1, 'Mycobacterium_tuberculosis_Rv1258c_1_V219A': 1, 'embB_1_D311H': 1, 'katG_1_.': 1, 'iniA_.': 2, 'embB_1_M306V': 1, 'mshA_1_.': 3, 'embB_2_Y333H': 1, 'embB_1_M306T': 1, 'gidB_G117E': 1, 'mfpA_.': 1, 'embB_1_R507G': 1, 'embB_1_M306L': 1, 'embR_.': 1, 'embB_1_Y334H': 1, 'pncA_W68S': 1, 'rpoB_1_.': 1, 'embB_1_R460C': 1, 'AAC_2___Ic_.': 2, 'gidB_E92A': 1, 'gidB_A183E': 1, 'Mycobacterium_tuberculosis_Rv1258c_1_.': 1, 'pncA_Q10P': 1, 'embB_1_Q497K': 1}\n"
          ]
        }
      ],
      "source": [
        "opt_n_estimators = [5,10,25]\n",
        "opt_max_depth = [10,25,50]\n",
        "opt_random_state = [2,8,16]\n",
        "opt_min_samples_split = [10,20,30]\n",
        "\n",
        "arr_acc_train = []\n",
        "arr_acc_test = []\n",
        "\n",
        "for p1,p2,p3,p4 in zip(\n",
        "    opt_n_estimators, \n",
        "    opt_max_depth, \n",
        "    opt_random_state, \n",
        "    opt_min_samples_split\n",
        "    ):\n",
        "\n",
        "  clf = RandomForestClassifier(n_estimators=p1,\n",
        "                              max_depth=p2,\n",
        "                              min_samples_split=p4,\n",
        "                              min_samples_leaf=2,\n",
        "                              min_split_gain=0.0,\n",
        "                              colsample_bytree=\"sqrt\",\n",
        "                              subsample=0.8,\n",
        "                              random_state=p3,\n",
        "                              classifier_type=\"gini\"\n",
        "                              )\n",
        "  \n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "      x, y, test_size=0.25, random_state=random.randint(1, 25)\n",
        "  )\n",
        "\n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  from sklearn import metrics\n",
        "  print(metrics.accuracy_score(y_train, clf.predict(x_train)))\n",
        "  print(metrics.accuracy_score(y_test, clf.predict(x_test)))\n",
        "  print(clf.feature_importances_)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch + KFOLD Optimization"
      ],
      "metadata": {
        "id": "HS-UWourTNaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_n_estimators = [5,10,25]\n",
        "opt_max_depth = [10,25,50]\n",
        "opt_random_state = [2,8,16]\n",
        "opt_min_samples_split = [10,20,30]\n",
        "\n",
        "arr_acc_train = []\n",
        "arr_acc_test = []\n",
        "result_type_gsearch_kcv_test = []\n",
        "result_type_gsearch_kcv_train = []\n",
        "for p1,p2,p3,p4 in zip(\n",
        "    opt_n_estimators, \n",
        "    opt_max_depth, \n",
        "    opt_random_state, \n",
        "    opt_min_samples_split\n",
        "    ):\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators=p1,\n",
        "                                max_depth=p2,\n",
        "                                min_samples_split=p4,\n",
        "                                min_samples_leaf=2,\n",
        "                                min_split_gain=0.0,\n",
        "                                colsample_bytree=\"sqrt\",\n",
        "                                subsample=0.8,\n",
        "                                random_state=p3,\n",
        "                                classifier_type=\"gini\"\n",
        "                                )\n",
        "    \n",
        "    # x_train, x_test, y_train, y_test = train_test_split(\n",
        "    #     x, y, test_size=0.25, random_state=random.randint(1, 25)\n",
        "    # )\n",
        "\n",
        "    #---------K-Fold_Crossvalidation---------\n",
        "    kfold_cv = KFold(n_splits=5, random_state=67, shuffle=True)\n",
        "    fold_acc_train = []\n",
        "    fold_acc_test = []\n",
        "    for i, (train_index, test_index) in enumerate(kfold_cv.split(x)):\n",
        "        # print(f\"Fold {i}:\")\n",
        "        # print(f\"  Train idx={train_index}\")\n",
        "        # print(f\"  Test idx={test_index}\")\n",
        "        x_train , x_test = x.iloc[train_index,:],x.iloc[test_index,:]\n",
        "        y_train , y_test = y[train_index] , y[test_index]\n",
        "\n",
        "        clf.fit(x_train, y_train)\n",
        "\n",
        "        from sklearn import metrics\n",
        "        print(\"-------MODEL with Parameter:\",p1,p2,p3,p4,f\"FOLD-{i}-------\")\n",
        "        n_fold_gridsearch_train_acc = metrics.accuracy_score(y_train, clf.predict(x_train))\n",
        "        n_fold_gridsearch_test_acc = metrics.accuracy_score(y_test, clf.predict(x_test))\n",
        "        print(n_fold_gridsearch_train_acc)\n",
        "        print(n_fold_gridsearch_test_acc)\n",
        "\n",
        "        fold_acc_train.append(n_fold_gridsearch_train_acc)\n",
        "        fold_acc_test.append(n_fold_gridsearch_test_acc)\n",
        "\n",
        "    result_type_gsearch_kcv_train.append(np.average(np.array(fold_acc_train)))\n",
        "    result_type_gsearch_kcv_test.append(np.average(np.array(fold_acc_test)))\n",
        "\n",
        "print(result_type_gsearch_kcv_train)\n",
        "print(result_type_gsearch_kcv_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y8CgHTUTM_N",
        "outputId": "266e196a-9aa2-430f-c3bd-a2d3bc993b99"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------MODEL with Parameter: 5 10 2 10 FOLD-0-------\n",
            "0.7131782945736435\n",
            "0.696969696969697\n",
            "-------MODEL with Parameter: 5 10 2 10 FOLD-1-------\n",
            "0.8062015503875969\n",
            "0.7272727272727273\n",
            "-------MODEL with Parameter: 5 10 2 10 FOLD-2-------\n",
            "0.8\n",
            "0.75\n",
            "-------MODEL with Parameter: 5 10 2 10 FOLD-3-------\n",
            "0.7923076923076923\n",
            "0.875\n",
            "-------MODEL with Parameter: 5 10 2 10 FOLD-4-------\n",
            "0.8384615384615385\n",
            "0.78125\n",
            "-------MODEL with Parameter: 10 25 8 20 FOLD-0-------\n",
            "0.7209302325581395\n",
            "0.696969696969697\n",
            "-------MODEL with Parameter: 10 25 8 20 FOLD-1-------\n",
            "0.813953488372093\n",
            "0.7272727272727273\n",
            "-------MODEL with Parameter: 10 25 8 20 FOLD-2-------\n",
            "0.823076923076923\n",
            "0.71875\n",
            "-------MODEL with Parameter: 10 25 8 20 FOLD-3-------\n",
            "0.7769230769230769\n",
            "0.875\n",
            "-------MODEL with Parameter: 10 25 8 20 FOLD-4-------\n",
            "0.8\n",
            "0.78125\n",
            "-------MODEL with Parameter: 25 50 16 30 FOLD-0-------\n",
            "0.7209302325581395\n",
            "0.696969696969697\n",
            "-------MODEL with Parameter: 25 50 16 30 FOLD-1-------\n",
            "0.813953488372093\n",
            "0.7272727272727273\n",
            "-------MODEL with Parameter: 25 50 16 30 FOLD-2-------\n",
            "0.823076923076923\n",
            "0.75\n",
            "-------MODEL with Parameter: 25 50 16 30 FOLD-3-------\n",
            "0.7769230769230769\n",
            "0.90625\n",
            "-------MODEL with Parameter: 25 50 16 30 FOLD-4-------\n",
            "0.8\n",
            "0.78125\n",
            "[0.7900298151460942, 0.7869767441860465, 0.7869767441860465]\n",
            "[0.7660984848484849, 0.7598484848484849, 0.7723484848484848]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D9fuiMEvoQaA",
        "N1Rhh13ASLSS",
        "1sxzbrFbSDWD",
        "ipEma7srJFAk"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}